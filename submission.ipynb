{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submission.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-ia19/Java_OPP/blob/master/submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "#from scipy.optimize import minimize_scalar\n"
      ],
      "metadata": {
        "id": "P3AM2SWLJwoh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR3e4g1xmboV"
      },
      "source": [
        "def estimate_rf(st,sp,num_window):\n",
        "  spike_times = np.nonzero(sp)[0]\n",
        "  spike_times = spike_times[spike_times > num_window]\n",
        "  num_spikes = len(spike_times)\n",
        "  avg = np.zeros([len(st),len(st[0])]) #,num_window))\n",
        "\n",
        "  for i in range(num_spikes):\n",
        "    # Find the indexes of the time window preceding the event\n",
        "    w_id = range(spike_times[i] - num_window-1, spike_times[i]-1)\n",
        "    # Add the stim from this window to the average\n",
        "    avg = avg + np.sum(st[:,:,w_id], axis =2)\n",
        "   \n",
        "\n",
        "\n",
        "  rf = (avg / num_spikes*num_window) #[:,:,0]\n",
        "\n",
        "  print(rf.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  return rf\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp8wSpXyON2R"
      },
      "source": [
        "#def estimate_nl(st, sp, rf, K):\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_pI1rAiixD6"
      },
      "source": [
        "\n",
        "def generate_spikes(N,rate,duration):\n",
        "  spikes = []\n",
        "  bin_size = 1/1000 #in s\n",
        "  prob_of_spike = rate * bin_size\n",
        "    \n",
        "  for n in range(N):\n",
        "    spike_trains = np.zeros(int(duration))\n",
        "    for i in range(int(duration)):\n",
        "      if np.random.rand() < prob_of_spike:\n",
        "        spike_trains[i] = 1\n",
        "    spikes.append(spike_trains)\n",
        "    #print(sum(spike_trains)/(duration/1000)) #check if parameter lambda is correct\n",
        "      \n",
        "  return np.array(spikes)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HhGXyQOoDpq"
      },
      "source": [
        "def mle_fr(spike_trains):\n",
        "    spike_count = np.sum(spike_trains, axis=1)\n",
        "    N, duration = spike_trains.shape\n",
        "    k = spike_count/(duration/1000)\n",
        "    neg_log_likelihood = lambda rate: -np.sum(k * np.log(rate) - np.log(scipy.special.factorial(k)) - rate)\n",
        "    rate = minimize_scalar(neg_log_likelihood, bounds=(0, 1000), method='bounded')\n",
        "    return rate.x\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5SxqrKvXoG6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}